{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manhbd-22022602/big_data_for_beginners/blob/main/MapReduce_Primer_HelloWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n",
        "\n",
        "Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"
      ],
      "metadata": {
        "id": "GzbmlR27wh6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download core Hadoop"
      ],
      "metadata": {
        "id": "uUbM5R0GwwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDgQtQlzw8bL",
        "outputId": "8b112e08-6512-4c52-bed2-6c4dce9716aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set environment variables"
      ],
      "metadata": {
        "id": "3yvb5cw9xEbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "metadata": {
        "id": "u6lkrz1dxIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7maAwaFxBT_",
        "outputId": "e86fdb07-9d83-47e9-84ee-0650207648b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."
      ],
      "metadata": {
        "id": "4kzJ8cNoxPyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SauFHVPOxL-Y",
        "outputId": "d94a57ef-f2ef-4ce1-c5e8-675d176f916c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "metadata": {
        "id": "6HFPVX84xbNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ],
      "metadata": {
        "id": "_yVa55X1xmOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Hello, World!\">./hello.txt"
      ],
      "metadata": {
        "id": "9Jz7mJkcxYxw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."
      ],
      "metadata": {
        "id": "zSh_Kr5Bxvst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb5JryK9xpPA",
        "outputId": "8022b6ad-9320-4dec-a872-400257df9332"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `my_output': No such file or directory\n",
            "2024-04-23 14:14:46,922 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 14:14:47,206 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 14:14:47,206 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 14:14:47,233 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 14:14:47,634 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 14:14:47,671 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 14:14:48,116 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local649564432_0001\n",
            "2024-04-23 14:14:48,116 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 14:14:48,381 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 14:14:48,384 INFO mapreduce.Job: Running job: job_local649564432_0001\n",
            "2024-04-23 14:14:48,394 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 14:14:48,398 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 14:14:48,408 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 14:14:48,408 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 14:14:48,474 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 14:14:48,481 INFO mapred.LocalJobRunner: Starting task: attempt_local649564432_0001_m_000000_0\n",
            "2024-04-23 14:14:48,551 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 14:14:48,552 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 14:14:48,581 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 14:14:48,598 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 14:14:48,624 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-23 14:14:48,721 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-23 14:14:48,721 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-23 14:14:48,722 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-23 14:14:48,722 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-23 14:14:48,722 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-23 14:14:48,727 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-23 14:14:48,735 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-23 14:14:48,748 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-23 14:14:48,751 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-23 14:14:48,751 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-23 14:14:48,752 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-23 14:14:48,752 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-23 14:14:48,753 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-23 14:14:48,755 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-23 14:14:48,755 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-23 14:14:48,762 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-23 14:14:48,763 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-23 14:14:48,764 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-23 14:14:48,765 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-23 14:14:48,801 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-23 14:14:48,803 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-23 14:14:48,803 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-23 14:14:48,804 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-23 14:14:48,807 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 14:14:48,807 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-23 14:14:48,807 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-23 14:14:48,807 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
            "2024-04-23 14:14:48,807 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-23 14:14:48,816 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-23 14:14:48,834 INFO mapred.Task: Task:attempt_local649564432_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 14:14:48,848 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-23 14:14:48,848 INFO mapred.Task: Task 'attempt_local649564432_0001_m_000000_0' done.\n",
            "2024-04-23 14:14:48,860 INFO mapred.Task: Final Counters for attempt_local649564432_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=854181\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=406847488\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-04-23 14:14:48,860 INFO mapred.LocalJobRunner: Finishing task: attempt_local649564432_0001_m_000000_0\n",
            "2024-04-23 14:14:48,862 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 14:14:48,870 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-23 14:14:48,871 INFO mapred.LocalJobRunner: Starting task: attempt_local649564432_0001_r_000000_0\n",
            "2024-04-23 14:14:48,893 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 14:14:48,893 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 14:14:48,894 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 14:14:48,901 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6667c0e5\n",
            "2024-04-23 14:14:48,905 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 14:14:48,933 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-23 14:14:48,936 INFO reduce.EventFetcher: attempt_local649564432_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-23 14:14:48,983 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local649564432_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n",
            "2024-04-23 14:14:48,990 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local649564432_0001_m_000000_0\n",
            "2024-04-23 14:14:48,994 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
            "2024-04-23 14:14:48,998 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-23 14:14:49,000 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 14:14:49,000 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-23 14:14:49,011 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 14:14:49,011 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-23 14:14:49,014 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-23 14:14:49,015 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n",
            "2024-04-23 14:14:49,016 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-23 14:14:49,016 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 14:14:49,017 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-23 14:14:49,018 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 14:14:49,035 INFO mapred.Task: Task:attempt_local649564432_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 14:14:49,037 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 14:14:49,037 INFO mapred.Task: Task attempt_local649564432_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-23 14:14:49,039 INFO output.FileOutputCommitter: Saved output of task 'attempt_local649564432_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-23 14:14:49,041 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-23 14:14:49,041 INFO mapred.Task: Task 'attempt_local649564432_0001_r_000000_0' done.\n",
            "2024-04-23 14:14:49,042 INFO mapred.Task: Final Counters for attempt_local649564432_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141992\n",
            "\t\tFILE: Number of bytes written=854231\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=406847488\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 14:14:49,042 INFO mapred.LocalJobRunner: Finishing task: attempt_local649564432_0001_r_000000_0\n",
            "2024-04-23 14:14:49,042 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-23 14:14:49,392 INFO mapreduce.Job: Job job_local649564432_0001 running in uber mode : false\n",
            "2024-04-23 14:14:49,393 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-23 14:14:49,395 INFO mapreduce.Job: Job job_local649564432_0001 completed successfully\n",
            "2024-04-23 14:14:49,408 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283906\n",
            "\t\tFILE: Number of bytes written=1708412\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=813694976\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 14:14:49,408 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ],
      "metadata": {
        "id": "OB_fX9u5x55y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnvEvYDfx2g4",
        "outputId": "354b5a7c-b1ef-466d-cc8b-1bb3cb5740b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "metadata": {
        "id": "BLMnBh44x_YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufAfmGUvx8jW",
        "outputId": "e84996e9-a9f6-4b90-906f-e1c17ce05015"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-23 14:14 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         15 2024-04-23 14:14 my_output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnKSahPzyCAn",
        "outputId": "7b41b4dd-0029-4fe4-822b-4b15564e38ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 15 Apr 23 14:14 part-00000\n",
            "-rw-r--r-- 1 root root  0 Apr 23 14:14 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "metadata": {
        "id": "v9LmpcaMyG23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL-Clat5yD8I",
        "outputId": "42e1a588-0727-4d0d-8b91-8ca69e097c80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "metadata": {
        "id": "AmpHr_HyyMnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mapred streaming -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWL1AiXyJac",
        "outputId": "8d7722b3-b221-4684-89fb-9360b3489f76"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-23 14:14:56,280 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H2MkIUPyQc2",
        "outputId": "eae7b64c-7ed9-40f6-b2af-badfffd774ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 14:14:58,499 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 14:15:00,800 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 14:15:00,981 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 14:15:00,981 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 14:15:01,009 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 14:15:01,333 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 14:15:01,376 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 14:15:01,736 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1654258893_0001\n",
            "2024-04-23 14:15:01,736 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 14:15:02,062 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 14:15:02,065 INFO mapreduce.Job: Running job: job_local1654258893_0001\n",
            "2024-04-23 14:15:02,084 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 14:15:02,088 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 14:15:02,100 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 14:15:02,100 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 14:15:02,182 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 14:15:02,193 INFO mapred.LocalJobRunner: Starting task: attempt_local1654258893_0001_m_000000_0\n",
            "2024-04-23 14:15:02,248 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 14:15:02,251 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 14:15:02,295 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 14:15:02,308 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 14:15:02,328 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-23 14:15:02,426 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-23 14:15:02,426 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-23 14:15:02,426 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-23 14:15:02,426 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-23 14:15:02,426 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-23 14:15:02,435 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-23 14:15:02,447 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 14:15:02,447 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-23 14:15:02,448 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-23 14:15:02,448 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
            "2024-04-23 14:15:02,448 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-23 14:15:02,456 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-23 14:15:02,474 INFO mapred.Task: Task:attempt_local1654258893_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 14:15:02,477 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-04-23 14:15:02,478 INFO mapred.Task: Task 'attempt_local1654258893_0001_m_000000_0' done.\n",
            "2024-04-23 14:15:02,489 INFO mapred.Task: Final Counters for attempt_local1654258893_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855525\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=399507456\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-04-23 14:15:02,489 INFO mapred.LocalJobRunner: Finishing task: attempt_local1654258893_0001_m_000000_0\n",
            "2024-04-23 14:15:02,491 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 14:15:02,499 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-23 14:15:02,500 INFO mapred.LocalJobRunner: Starting task: attempt_local1654258893_0001_r_000000_0\n",
            "2024-04-23 14:15:02,516 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 14:15:02,516 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 14:15:02,516 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 14:15:02,524 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@67fa1caf\n",
            "2024-04-23 14:15:02,527 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 14:15:02,553 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-23 14:15:02,559 INFO reduce.EventFetcher: attempt_local1654258893_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-23 14:15:02,618 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1654258893_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
            "2024-04-23 14:15:02,632 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local1654258893_0001_m_000000_0\n",
            "2024-04-23 14:15:02,636 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
            "2024-04-23 14:15:02,641 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-23 14:15:02,643 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 14:15:02,643 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-23 14:15:02,653 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 14:15:02,654 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-04-23 14:15:02,656 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-23 14:15:02,657 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
            "2024-04-23 14:15:02,658 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-23 14:15:02,658 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 14:15:02,659 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-04-23 14:15:02,660 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 14:15:02,671 INFO mapred.Task: Task:attempt_local1654258893_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 14:15:02,673 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 14:15:02,673 INFO mapred.Task: Task attempt_local1654258893_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-23 14:15:02,677 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1654258893_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-23 14:15:02,678 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-23 14:15:02,678 INFO mapred.Task: Task 'attempt_local1654258893_0001_r_000000_0' done.\n",
            "2024-04-23 14:15:02,679 INFO mapred.Task: Final Counters for attempt_local1654258893_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142006\n",
            "\t\tFILE: Number of bytes written=855583\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=399507456\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 14:15:02,679 INFO mapred.LocalJobRunner: Finishing task: attempt_local1654258893_0001_r_000000_0\n",
            "2024-04-23 14:15:02,680 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-23 14:15:03,080 INFO mapreduce.Job: Job job_local1654258893_0001 running in uber mode : false\n",
            "2024-04-23 14:15:03,081 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-23 14:15:03,083 INFO mapreduce.Job: Job job_local1654258893_0001 completed successfully\n",
            "2024-04-23 14:15:03,094 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283920\n",
            "\t\tFILE: Number of bytes written=1711108\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=799014912\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 14:15:03,098 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "v7Ks3e96yXuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWAXvG0_yThc",
        "outputId": "a44c0180-2fc5-4cae-bf15-b25051863128"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "t40GgJ2Hya9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5APWEgoyaRS",
        "outputId": "f0e9ddc9-6312-42df-a96b-d50092e6ac52"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "metadata": {
        "id": "mzfaMVKqyjpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "metadata": {
        "id": "lzIuWv7Myndc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdwKWyVRye27",
        "outputId": "357b6e4d-d407-4b08-e8c1-3124f3eb72ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 14:15:08,279 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 14:15:11,269 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 14:15:11,479 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 14:15:11,480 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 14:15:11,507 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 14:15:11,792 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 14:15:11,821 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 14:15:12,188 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1866119678_0001\n",
            "2024-04-23 14:15:12,188 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 14:15:12,508 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 14:15:12,510 INFO mapreduce.Job: Running job: job_local1866119678_0001\n",
            "2024-04-23 14:15:12,525 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 14:15:12,537 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 14:15:12,546 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 14:15:12,546 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 14:15:12,649 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 14:15:12,656 INFO mapred.LocalJobRunner: Starting task: attempt_local1866119678_0001_m_000000_0\n",
            "2024-04-23 14:15:12,702 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 14:15:12,704 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 14:15:12,751 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 14:15:12,767 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 14:15:12,788 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-23 14:15:12,824 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 14:15:12,839 INFO mapred.Task: Task:attempt_local1866119678_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 14:15:12,843 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 14:15:12,843 INFO mapred.Task: Task attempt_local1866119678_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-23 14:15:12,853 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1866119678_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-23 14:15:12,858 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-04-23 14:15:12,861 INFO mapred.Task: Task 'attempt_local1866119678_0001_m_000000_0' done.\n",
            "2024-04-23 14:15:12,874 INFO mapred.Task: Final Counters for attempt_local1866119678_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855489\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=373293056\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 14:15:12,874 INFO mapred.LocalJobRunner: Finishing task: attempt_local1866119678_0001_m_000000_0\n",
            "2024-04-23 14:15:12,877 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 14:15:13,537 INFO mapreduce.Job: Job job_local1866119678_0001 running in uber mode : false\n",
            "2024-04-23 14:15:13,539 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-23 14:15:13,543 INFO mapreduce.Job: Job job_local1866119678_0001 completed successfully\n",
            "2024-04-23 14:15:13,551 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855489\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=373293056\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 14:15:13,551 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "QZIE9yXOyyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dt3tUI0yu5e",
        "outputId": "d27cd633-c8c8-41ac-9c1b-623e3f3b447b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why a map-only application?\n",
        "\n",
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"
      ],
      "metadata": {
        "id": "hUGEUv99y3cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "metadata": {
        "id": "FhVVFEdKzGcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLgMXX2jy0vC",
        "outputId": "e38e5956-f1d8-49d9-affb-8b32d772460f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 14:15:17,773 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 14:15:20,711 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 14:15:21,168 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 14:15:21,168 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 14:15:21,229 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 14:15:21,871 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 14:15:21,925 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 14:15:22,522 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1381149041_0001\n",
            "2024-04-23 14:15:22,522 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 14:15:22,956 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 14:15:22,958 INFO mapreduce.Job: Running job: job_local1381149041_0001\n",
            "2024-04-23 14:15:22,991 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 14:15:22,994 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 14:15:23,007 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 14:15:23,008 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 14:15:23,073 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 14:15:23,079 INFO mapred.LocalJobRunner: Starting task: attempt_local1381149041_0001_m_000000_0\n",
            "2024-04-23 14:15:23,120 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 14:15:23,121 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 14:15:23,157 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 14:15:23,171 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 14:15:23,193 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-23 14:15:23,210 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-23 14:15:23,218 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-23 14:15:23,220 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-23 14:15:23,220 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-23 14:15:23,221 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-23 14:15:23,221 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-23 14:15:23,222 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-23 14:15:23,224 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-23 14:15:23,224 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-23 14:15:23,224 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-23 14:15:23,225 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-23 14:15:23,226 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-23 14:15:23,227 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-23 14:15:23,250 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-23 14:15:23,256 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-23 14:15:23,261 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-23 14:15:23,261 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-23 14:15:23,266 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 14:15:23,280 INFO mapred.Task: Task:attempt_local1381149041_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 14:15:23,283 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 14:15:23,283 INFO mapred.Task: Task attempt_local1381149041_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-23 14:15:23,286 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1381149041_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-23 14:15:23,287 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-23 14:15:23,287 INFO mapred.Task: Task 'attempt_local1381149041_0001_m_000000_0' done.\n",
            "2024-04-23 14:15:23,304 INFO mapred.Task: Final Counters for attempt_local1381149041_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=858459\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=363855872\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 14:15:23,304 INFO mapred.LocalJobRunner: Finishing task: attempt_local1381149041_0001_m_000000_0\n",
            "2024-04-23 14:15:23,306 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 14:15:23,999 INFO mapreduce.Job: Job job_local1381149041_0001 running in uber mode : false\n",
            "2024-04-23 14:15:24,002 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-23 14:15:24,006 INFO mapreduce.Job: Job job_local1381149041_0001 completed successfully\n",
            "2024-04-23 14:15:24,017 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=858459\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=363855872\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 14:15:24,017 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa1UDPr6zKKw",
        "outputId": "d36b9fbd-53aa-4d23-ef24-0e5653d29603"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    }
  ]
}